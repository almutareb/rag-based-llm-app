{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2El6e7/+ehu6RNm3T0tSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/rag-based-llm-app/blob/main/Mistral_7b_RAG%2BChroma_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install needed packages and libraries"
      ],
      "metadata": {
        "id": "9OZHVC9W9Vvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf5fT_2bwKWY"
      },
      "outputs": [],
      "source": [
        "### Installation on Colab (As Of Oct)\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U einops\n",
        "!pip install -q -U safetensors\n",
        "!pip install -q -U torch==2.0.1\n",
        "!pip install -q -U xformers\n",
        "!pip install -q -U langchain\n",
        "!pip install -q -U ctransformers[cuda]\n",
        "!pip install chromadb\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG) with Mistral-7B-Instruct and Chroma DB."
      ],
      "metadata": {
        "id": "4fQmvZQT9bgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"auto\",quantization_config=quantization_config, )\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_4bit,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=500,\n",
        "        do_sample=True,\n",
        "        top_k=5,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "#### Prompt\n",
        "template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Answer exactly in few words from the context\n",
        "Answer the question below from context below :\n",
        "{context}\n",
        "{question} [/INST] </s>\n",
        "\"\"\"\n",
        "\n",
        "question_p = \"\"\"What is the date for announcement\"\"\"\n",
        "context_p = \"\"\" On August 10 said that its arm JSW Neo Energy has agreed to buy a portfolio of 1753 mega watt renewable energy generation capacity from Mytrah Energy India Pvt Ltd for Rs 10,530 crore.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run({\"question\":question_p,\"context\":context_p})\n",
        "response"
      ],
      "metadata": {
        "id": "MQNFZHe7yfOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add vectore store and embeddings"
      ],
      "metadata": {
        "id": "wI-OL193xcyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "mna_news = \"\"\"In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study\n",
        "the vision tasks with data in multiple modalities. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their inherited weaknesses in performing precise mathematical calculation, multi-step logic reasoning, perception\n",
        "about the spatial and topological factors, and handling the temporal progression. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing\n",
        "LLMs with the graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose\n",
        "the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically,\n",
        "we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging\n",
        "from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic paper citation networks, protein molecular\n",
        "graphs, sequential recommender systems, online social networks and knowledge graphs.\n",
        "Technically, to build Graph-ToolFormer, we propose to handcraft both the instruction and a small amount of prompt templates for each of the graph reasoning tasks, respectively. Via in-context\n",
        "learning, based on such instructions and prompt template examples, we adopt ChatGPT to annotate and augment a larger graph reasoning statement dataset with the most appropriate calls of external API\n",
        "functions. Such augmented prompt datasets will be post-processed with selective filtering and used for fine-tuning existing pre-trained causal LLMs, such as the GPT-J and LLaMA, to teach them how to\n",
        "use graph reasoning tools in the output generation. To demonstrate the effectiveness of Graph-ToolFormer, we conduct extensive experimental studies on various graph reasoning datasets and tasks,\n",
        "and have also launched a LLM demo with various graph reasoning abilities. All the source code of Graph-ToolFormer framework, the demo for graph reasoning, and the graph and prompt datasets have been released online at the project github page.\n",
        "\"\"\"\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "documents = [Document(page_content=mna_news, metadata={\"source\": \"local\"})]\n",
        "\n",
        "#######################\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "#######################\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "#######################\n",
        "retriever = vectordb.as_retriever()\n",
        "#######################\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def run_my_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    result = qa.run(query)\n",
        "    print(\"\\nResult: \", result)\n",
        "\n",
        "### Ask Queries Now\n",
        "query =\"\"\" What is the focus of this paper and what are the main findings? \"\"\"\n",
        "run_my_rag(qa, query)"
      ],
      "metadata": {
        "id": "t07Cctg-7AVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GGUF format for commodity hardware (Running on CPU)."
      ],
      "metadata": {
        "id": "Ka1yyFqjKiPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from transformers import BitsAndBytesConfig\n",
        "#from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "from langchain.llms import CTransformers\n",
        "config = {'max_new_tokens': 100, 'temperature': 0}\n",
        "llm = CTransformers(model='TheBloke/Mistral-7B-Instruct-v0.1-GGUF',model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", config=config)\n",
        "\n",
        "template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Answer exactly in few words from the context\n",
        "Answer the question below from context below :\n",
        "{context}\n",
        "{question} [/INST] </s>\n",
        "\"\"\"\n",
        "\n",
        "#### Prompt\n",
        "question_p = \"\"\"What approach was used in the paper?\"\"\"\n",
        "context_p = \"\"\" Technically, to build Graph-ToolFormer, we propose to handcraft both the instruction and a small amount of prompt templates for each of the graph reasoning tasks, respectively. Via in-context\n",
        "learning, based on such instructions and prompt template examples, we adopt ChatGPT to annotate and augment a larger graph reasoning statement dataset with the most appropriate calls of external API\n",
        "functions. Such augmented prompt datasets will be post-processed with selective filtering and used for fine-tuning existing pre-trained causal LLMs, such as the GPT-J and LLaMA, to teach them how to\n",
        "use graph reasoning tools in the output generation. To demonstrate the effectiveness of Graph-ToolFormer, we conduct extensive experimental studies on various graph reasoning datasets and tasks,\n",
        "and have also launched a LLM demo with various graph reasoning abilities. All the source code of Graph-ToolFormer framework, the demo for graph reasoning, and the graph and prompt datasets have been released online at the project github page.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run({\"question\":question_p,\"context\":context_p})\n",
        "response"
      ],
      "metadata": {
        "id": "GmOk8V7F7MeZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}