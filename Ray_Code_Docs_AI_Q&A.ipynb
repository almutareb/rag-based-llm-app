{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPun3y2R3mvIKviBM/9tmCR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/rag-based-llm-app/blob/main/Ray_Code_Docs_AI_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J40YghyAKOm8"
      },
      "outputs": [],
      "source": [
        "# install needed packages\n",
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers typing-extensions==4.8.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the 'bitsandbytes' library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing model\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n"
      ],
      "metadata": {
        "id": "9FgUee6IFMCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stop tokens to controll output\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# initialize a text-generation transformer pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    # pass the following model parameters\n",
        "    # ensure the model doesn't ramble during chat\n",
        "    #stopping_criteria=stopping_criteria,\n",
        "    # max number of tokens to generate in the output\n",
        "    max_new_tokens=2048,\n",
        "    # limit repition in the output\n",
        "    repetition_penalty=1.2\n",
        ")"
      ],
      "metadata": {
        "id": "JEfSG8iPFZrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now with a HF pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "llm(prompt=\"Explain to me the difference between Data Lakehouse and Data Warehouse\")"
      ],
      "metadata": {
        "id": "xFRKzFA9Fknb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xo0ROrIHF3wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "\n",
        "url = \"https://docs.ray.io/en/master/\"\n",
        "loader = RecursiveUrlLoader(url=url, max_depth=4, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "M-A4KPOEwTZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "EhQbZhoyMUcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the documents into chunks with a small overlap\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=150,\n",
        "    #separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        "    )\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "mrlxRNNNDwy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_splits)"
      ],
      "metadata": {
        "id": "yNGgKUz4MZsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# use all-mpnet-base-v2 sentence transformer to convert pieces of text in vectors to store them in the vector store\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs\n",
        "    )\n",
        "\n",
        "vectorstores = FAISS.from_documents(all_splits, embeddings)\n",
        "FAISS_INDEX_PATH = \"faiss_index\"\n",
        "vectorstores.save_local(FAISS_INDEX_PATH)"
      ],
      "metadata": {
        "id": "VIfOPVIUEIij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a conversation chain, a summary buffer memory for chat history\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    vectorstores.as_retriever(search_type = \"mmr\"), # Maximum marginal relevance (mmr) strives to achieve both relevance to the query and diversity among the results.\n",
        "    return_source_documents=True\n",
        "    )"
      ],
      "metadata": {
        "id": "-jH-lRhBHWJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colab_print(text, max_width = 120):\n",
        "  words = text.split()\n",
        "  line = \"\"\n",
        "  for word in words:\n",
        "    if len(line) + len(word) + 1 > max_width:\n",
        "      print(line)\n",
        "      line = \"\"\n",
        "    line += word + \" \"\n",
        "  print (line)"
      ],
      "metadata": {
        "id": "TtayPYOmRRa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add a 'chat history' for testing\n",
        "# should use langchain's ChatMessageHistory instead\n",
        "chat_history = []\n",
        "\n",
        "query = \"What are placement groups?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "sources = [doc.metadata.get(\"source\") for doc in result['source_documents']]\n",
        "src_list = '\\n'.join(sources)\n",
        "\n",
        "colab_print(result['answer'] + src_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMn4CDssHXDB",
        "outputId": "badaa85a-53fd-4ca2-e16e-6e01c8244005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Placement groups allow users to atomically reserve groups of resources across multiple nodes (i.e., gang scheduling). \n",
            "They can be then used to schedule Ray tasks and actors packed as close as possible for locality (PACK), or spread apart \n",
            "(SPREAD).https://docs.ray.io/en/master/_sources/ray-core/scheduling/placement-group.rst.txt \n",
            "https://docs.ray.io/en/master/_sources/ray-core/scheduling/placement-group.rst.txt \n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html \n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sources = [doc.metadata.get(\"source\") for doc in result['source_documents']]\n",
        "print(*sources, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af8BSRMVVUeT",
        "outputId": "5c1e6e22-1bc4-4d8c-a8b3-eb58d9de603d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.ray.io/en/master/_sources/ray-core/scheduling/placement-group.rst.txt\n",
            "https://docs.ray.io/en/master/_sources/ray-core/scheduling/placement-group.rst.txt\n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html\n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source documents returned by FAISS\n",
        "# TODO: remove duplicates, there is a langchain tutorial where the returned unique sources -> look for it!\n",
        "print(*result['source_documents'], sep='\\n')"
      ],
      "metadata": {
        "id": "lgtmSuRXHZt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "\n",
        "query = \"how can I verify if the new placement group is pending creation?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "sources = [doc.metadata.get(\"source\") for doc in result['source_documents']]\n",
        "src_list = '\\n'.join(sources)\n",
        "\n",
        "colab_print(result['answer'] + src_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu9UtDLad8zI",
        "outputId": "1414c69c-9378-4dfa-8b95-ed8e54113eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can use the `ray list placement-groups` command to check the state of your placement groups. The output will show \n",
            "the name of each placement group along with its ID, creator job ID, and state. Look for the placement group you want to \n",
            "check and see if it has a state of \n",
            "`PENDING`.https://docs.ray.io/en/master/_sources/ray-core/scheduling/placement-group.rst.txt \n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html \n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html \n",
            "https://docs.ray.io/en/master/ray-core/scheduling/placement-group.html \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "\n",
        "query = \"how do I do that in java?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "sources = [doc.metadata.get(\"source\") for doc in result['source_documents']]\n",
        "src_list = '\\n'.join(sources)\n",
        "\n",
        "colab_print(result['answer'] + src_list)"
      ],
      "metadata": {
        "id": "eiH2kx_wezt-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}