{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXnRhrfbF37YwVOwjnSqcl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/rag-based-llm-app/blob/main/RAG_Arxiv_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv"
      ],
      "metadata": {
        "id": "iJ5ikBXgv0L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test to query and chat with Arxiv articles using an LLM\n",
        "First we will use OpenAI's ChatGPT-3.5-turbo, then try other options"
      ],
      "metadata": {
        "id": "nKg_nTdYtDCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8x0LbAsoGOa"
      },
      "outputs": [],
      "source": [
        "!pip install langchain arxiv openai python-dotenv pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ArxivRetriever"
      ],
      "metadata": {
        "id": "0_IQt5Mrs_vk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ArxivRetriever has these arguments:\n",
        "\n",
        "  * optional load_max_docs: default=100. Use it to limit number of downloaded documents. It takes time to download all 100 documents, so use a small number for experiments. There is a hard limit of 300 for now.\n",
        "  * optional load_all_available_meta: default=False. By default only the most important fields downloaded: Published (date when document was published/last updated), Title, Authors, Summary. If True, other fields also downloaded.\n",
        "\n",
        "get_relevant_documents() has one argument, query: free text which used to find documents in Arxiv.org"
      ],
      "metadata": {
        "id": "iI2pN_Optf8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = ArxivRetriever(load_max_docs=25)"
      ],
      "metadata": {
        "id": "d9b1XNjftrNH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(query=\"2310.03184\")"
      ],
      "metadata": {
        "id": "PDtPpUvLtyFQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKMeqdSxuKhi",
        "outputId": "bbf9e702-b363-42bb-be43-8f1b4cce5d30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Published': '2023-10-04',\n",
              " 'Title': 'Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference',\n",
              " 'Authors': 'Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, Wanli Xing',\n",
              " 'Summary': \"For middle-school math students, interactive question-answering (QA) with\\ntutors is an effective way to learn. The flexibility and emergent capabilities\\nof generative large language models (LLMs) has led to a surge of interest in\\nautomating portions of the tutoring process - including interactive QA to\\nsupport conceptual discussion of mathematical concepts. However, LLM responses\\nto math questions can be incorrect or mismatched to the educational context -\\nsuch as being misaligned with a school's curriculum. One potential solution is\\nretrieval-augmented generation (RAG), which involves incorporating a vetted\\nexternal knowledge source in the LLM prompt to increase response quality. In\\nthis paper, we designed prompts that retrieve and use content from a\\nhigh-quality open-source math textbook to generate responses to real student\\nquestions. We evaluate the efficacy of this RAG system for middle-school\\nalgebra and geometry QA by administering a multi-condition survey, finding that\\nhumans prefer responses generated using RAG, but not when responses are too\\ngrounded in the textbook content. We argue that while RAG is able to improve\\nresponse quality, designers of math QA systems must consider trade-offs between\\ngenerating responses preferred by students and responses closely matched to\\nspecific educational resources.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "DH5Ktk0AuQyz",
        "outputId": "47bf2d98-9f33-4428-bfd8-16475c29063e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval-augmented Generation to Improve Math\\nQuestion-Answering: Trade-offs Between\\nGroundedness and Human Preference\\nZachary Levonian1\\nChenglu Li2\\nWangda Zhu3\\nAnoushka Gade1\\nOwen Henkel4\\nMillie-Ellen Postle5\\nWanli Xing3\\n1Digital Harbor Foundation\\n2University of Utah\\n3University of Florida\\n4University of Oxford\\n5Rising Academies\\nzach@digitalharbor.org\\nAbstract\\nFor middle-school math students, interactive question-answering (QA) with tutors\\nis an effective way to learn. The flexibility and emer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A with ChatGPT-3.5"
      ],
      "metadata": {
        "id": "X_p4q5wQv4XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ],
      "metadata": {
        "id": "mH4VzLF4wGzn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "qs = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
      ],
      "metadata": {
        "id": "KZDwKgXAwU-P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are the challenges of using LLMs to generate math answers for middle-school students?\",\n",
        "    \"How does retrieval-augmented generation (RAG) address these challenges?\",\n",
        "    \"What is the design of the RAG system used in this paper?\",\n",
        "    \"How was the efficacy of the RAG system evaluated?\",\n",
        "    \"What are the trade-offs between generating responses preferred by students and responses closely matched to specific educational resources?\",\n",
        "]\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "for question in questions:\n",
        "  result = qs({\"question\": question, \"chat_history\": chat_history})\n",
        "  chat_history.append((question, result[\"answer\"]))\n",
        "  print(f\"-> **Question**: {question} \\n\")\n",
        "  print(f\"**Answer**: {result['answer']} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRCCCYN8w2Rk",
        "outputId": "3f1fa496-b82c-48cf-e4fa-4b3b6109ee95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: What are the challenges of using LLMs to generate math answers for middle-school students? \n",
            "\n",
            "**Answer**: The given context does not specifically mention the challenges of using LLMs to generate math answers for middle-school students. Therefore, based on the provided context, it is not possible to determine the challenges related to this specific use case. \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: How does retrieval-augmented generation (RAG) address these challenges? \n",
            "\n",
            "**Answer**: La génération augmentée par recherche (RAG) est une approche qui combine les capacités de la recherche d'informations avec la génération de texte pour aborder les défis de la génération de texte. RAG utilise des modèles de langage pré-entraînés et des techniques de recherche d'informations pour fournir des réponses plus précises et informatives. Par exemple, au lieu de générer simplement une réponse à partir de zéro, RAG peut effectuer une recherche sur une base de connaissances ou sur le web pour obtenir des informations supplémentaires et les intégrer dans la réponse générée. Cela permet d'améliorer la qualité et la pertinence des réponses générées. Cependant, il est important de noter que RAG est une technologie en développement et peut avoir des limites et des lacunes dans sa capacité à aborder tous les défis de manière optimale. \n",
            "\n",
            "-> **Question**: What is the design of the RAG system used in this paper? \n",
            "\n",
            "**Answer**: Je suis désolé, mais je ne suis pas en mesure de fournir une réponse à votre question car je n'ai pas accès à l'article en question et je ne suis pas programmé pour connaître les détails spécifiques du design du système RAG mentionné. \n",
            "\n",
            "-> **Question**: How was the efficacy of the RAG system evaluated? \n",
            "\n",
            "**Answer**: L'efficacité du système RAG (Red, Amber, Green) a été évaluée à travers plusieurs études et recherches. Ces évaluations se sont basées sur des critères tels que la facilité d'utilisation du système, la précision des résultats obtenus, la satisfaction des utilisateurs, et l'impact du système sur la prise de décision. Des données quantitatives et qualitatives ont été collectées pour évaluer ces différents aspects. Cependant, les détails spécifiques de ces évaluations peuvent varier en fonction du contexte d'utilisation du système RAG. \n",
            "\n",
            "-> **Question**: What are the trade-offs between generating responses preferred by students and responses closely matched to specific educational resources? \n",
            "\n",
            "**Answer**: Les compromis entre la génération de réponses préférées par les élèves et les réponses étroitement correspondantes à des ressources éducatives spécifiques peuvent varier en fonction de plusieurs facteurs. \n",
            "\n",
            "D'une part, les réponses préférées par les élèves peuvent être basées sur leurs intérêts personnels, leur niveau de compréhension et leurs préférences d'apprentissage. Ces réponses peuvent être plus engageantes et motivantes pour les élèves, ce qui peut favoriser leur participation et leur apprentissage.\n",
            "\n",
            "D'autre part, les réponses étroitement correspondantes à des ressources éducatives spécifiques peuvent être plus alignées sur les objectifs d'apprentissage et les compétences visées par le programme scolaire. Ces réponses peuvent fournir des informations précises et structurées, en se basant sur des sources fiables et validées.\n",
            "\n",
            "Les compromis peuvent inclure :\n",
            "\n",
            "- L'adaptation des réponses préférées par les élèves pour intégrer des éléments issus de ressources éducatives spécifiques, afin de répondre aux exigences académiques tout en maintenant l'intérêt des élèves.\n",
            "- L'utilisation de ressources éducatives spécifiques pour compléter ou approfondir les réponses préférées par les élèves, afin de garantir un contenu éducatif solide et rigoureux.\n",
            "- L'équilibre entre l'autonomie des élèves dans la génération de leurs réponses préférées et l'encadrement pédagogique nécessaire pour s'assurer de la pertinence et de la précision des informations fournies.\n",
            "\n",
            "Cependant, il est important de noter que les compromis peuvent varier en fonction du contexte éducatif spécifique, des ressources disponibles et des priorités pédagogiques. Par conséquent, il n'y a pas de réponse unique à cette question et les compromis doivent être adaptés à chaque situation. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using an agent"
      ],
      "metadata": {
        "id": "2D1NA2CNSPXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0)\n",
        "tools = load_tools(\n",
        "    [\"arxiv\"],\n",
        ")\n",
        "\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QMt1_lGwSRjo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(\n",
        "    \"What papers cover the use of RAG to generate math answers for middle-school students?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZdN1wrpESqmL",
        "outputId": "6f73e729-c200-4b51-ef64-604bda1e36b8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should search for papers on arxiv.org that discuss the use of RAG (Retrieve, Answer, Generate) models in generating math answers for middle-school students.\n",
            "Action: arxiv\n",
            "Action Input: \"RAG model for generating math answers middle-school students\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-10-04\n",
            "Title: Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference\n",
            "Authors: Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, Wanli Xing\n",
            "Summary: For middle-school math students, interactive question-answering (QA) with\n",
            "tutors is an effective way to learn. The flexibility and emergent capabilities\n",
            "of generative large language models (LLMs) has led to a surge of interest in\n",
            "automating portions of the tutoring process - including interactive QA to\n",
            "support conceptual discussion of mathematical concepts. However, LLM responses\n",
            "to math questions can be incorrect or mismatched to the educational context -\n",
            "such as being misaligned with a school's curriculum. One potential solution is\n",
            "retrieval-augmented generation (RAG), which involves incorporating a vetted\n",
            "external knowledge source in the LLM prompt to increase response quality. In\n",
            "this paper, we designed prompts that retrieve and use content from a\n",
            "high-quality open-source math textbook to generate responses to real student\n",
            "questions. We evaluate the efficacy of this RAG system for middle-school\n",
            "algebra and geometry QA by administering a multi-condition survey, finding that\n",
            "humans prefer responses generated using RAG, but not when responses are too\n",
            "grounded in the textbook content. We argue that while RAG is able to improve\n",
            "response quality, designers of math QA systems must consider trade-offs between\n",
            "generating responses preferred by students and responses closely matched to\n",
            "specific educational resources.\n",
            "\n",
            "Published: 2022-10-06\n",
            "Title: Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\n",
            "Authors: Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, Suranga Nanayakkara\n",
            "Summary: Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain\n",
            "Question Answering (ODQA). RAG has only been trained and explored with a\n",
            "Wikipedia-based external knowledge base and is not optimized for use in other\n",
            "specialized domains such as healthcare and news. In this paper, we evaluate the\n",
            "impact of joint training of the retriever and generator components of RAG for\n",
            "the task of domain adaptation in ODQA. We propose \\textit{RAG-end2end}, an\n",
            "extension to RAG, that can adapt to a domain-specific knowledge base by\n",
            "updating all components of the external knowledge base during training. In\n",
            "addition, we introduce an auxiliary training signal to inject more\n",
            "domain-specific knowledge. This auxiliary signal forces \\textit{RAG-end2end} to\n",
            "reconstruct a given sentence by accessing the relevant information from the\n",
            "external knowledge base. Our novel contribution is unlike RAG, RAG-end2end does\n",
            "joint training of the retriever and generator for the end QA task and domain\n",
            "adaptation. We evaluate our approach with datasets from three domains:\n",
            "COVID-19, News, and Conversations, and achieve significant performance\n",
            "improvements compared to the original RAG model. Our work has been open-sourced\n",
            "through the Huggingface Transformers library, attesting to our work's\n",
            "credibility and technical consistency.\n",
            "\n",
            "Published: 2021-06-22\n",
            "Title: Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering\n",
            "Authors: Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Suranga Nanayakkara\n",
            "Summary: In this paper, we illustrate how to fine-tune the entire Retrieval Augment\n",
            "Generation (RAG) architecture in an end-to-end manner. We highlighted the main\n",
            "engineering challenges that needed to be addressed to achieve this objective.\n",
            "We also compare how end-to-end RAG architecture outperforms the original RAG\n",
            "architecture for the task of question answering. We have open-sourced our\n",
            "implementation in the HuggingFace Transformers library.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have found several papers that discuss the use of RAG models in generating math answers for middle-school students. The papers include \"Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference\" by Zachary Levonian et al. (2023), \"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\" by Shamane Siriwardhana et al. (2022), and \"Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering\" by Shamane Siriwardhana et al. (2021).\n",
            "Final Answer: The papers \"Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference\" (2023), \"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\" (2022), and \"Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering\" (2021) cover the use of RAG to generate math answers for middle-school students.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The papers \"Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference\" (2023), \"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\" (2022), and \"Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering\" (2021) cover the use of RAG to generate math answers for middle-school students.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert from HuggingFace"
      ],
      "metadata": {
        "id": "LTxzTPbc0GZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1K7rBERf0JkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"distilgpt2\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\"temperature\":0, \"max_length\":256},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWL5SbX00iaV",
        "outputId": "08001cee-9b20-4f31-8214-fb8b3ecdf0fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step. \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "question = \"What are the challenges of using LLMs to generate math answers for middle-school students?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "kL6UovNY4uXZ",
        "outputId": "c60809e3-4ac0-453d-883c-b31ea37b4201"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-57d75a8f2961>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What are the challenges of using LLMs to generate math answers for middle-school students?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   1114\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         return (\n\u001b[0;32m--> 230\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    497\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 )\n\u001b[1;32m    646\u001b[0m             ]\n\u001b[0;32m--> 647\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             output = (\n\u001b[0;32m--> 522\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;31m# Process batch of prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# Process each response in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcan_use_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m                 final_iterator = self.get_iterator(\n\u001b[0m\u001b[1;32m   1119\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mget_iterator\u001b[0;34m(self, inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# TODO hack by collating feature_extractor and image_processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_collate_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpad_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mmodel_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mpad_collate_fn\u001b[0;34m(tokenizer, feature_extractor)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;34m\"Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;34m\"`pipe.tokenizer.pad_token_id = model.config.eos_token_id`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with `pipe.tokenizer.pad_token_id = model.config.eos_token_id`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain"
      ],
      "metadata": {
        "id": "qDpJeWgISAVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new approach"
      ],
      "metadata": {
        "id": "8h_3yW_HYBve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ispp_qujYDKP",
        "outputId": "5e54249b-4cfc-4024-808a-45e105392d5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "hf_auth = os.environ['HF_API_KEY']"
      ],
      "metadata": {
        "id": "2cNQPh-saR__"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "#model_id = 'distilbert-base-cased-distilled-squad'\n",
        "model_id = 'Open-Orca/Mistral-7B-OpenOrca'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "#model = transformers.AutoModelForQuestionAnswering.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Zbs8BOhyYnj2",
        "outputId": "c0665a14-7911-41cb-8e98-1642f89c3b60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6bc812e896fa>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m model_config = transformers.AutoConfig.from_pretrained(\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_auth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hf_auth' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "u5FjK8KbcsiX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopping criteria\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BtcxpPndqiK",
        "outputId": "dfc6de49-4b34-4fa8-f852-502cb7b7960a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[198, 20490, 25], [198, 15506, 63, 198]]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert stop token ids into LongTensor objects\n",
        "\n",
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ExrPLeOeO33",
        "outputId": "1019a116-fd65-4a8e-ff64-42374c4fc050"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([  198, 20490,    25], device='cuda:0'),\n",
              " tensor([  198, 15506,    63,   198], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# object that will check whether the stopping criteria has been satisfied\n",
        "\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "    for stop_ids in stop_token_ids:\n",
        "      if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "TYFKdQt1ezcl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the HF pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # langchain expects the full text\n",
        "    return_full_text=True,\n",
        "    #task='question-answering',\n",
        "    task='text-generation',\n",
        "    # pass model parameters\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.1,\n",
        "    # max tokens in generated output\n",
        "    max_new_tokens=512,\n",
        "    # penalize repetition to reduce it\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "GcHdhCC4f8aX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain to me the difference between ETF and managed Fonds\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDL9ReJFgu4U",
        "outputId": "cfe6d9ee-8265-4252-b82c-0e7ade7574ee"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain to me the difference between ETF and managed Fonds.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
          ]
        }
      ]
    }
  ]
}